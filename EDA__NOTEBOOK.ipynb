{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff72047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(references, candidates):\n",
    "    scores = {}\n",
    "\n",
    "    # --- ROUGE ---\n",
    "    rouge_result = rouge.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    scores.update({f\"{k.upper()}\": round(v, 4) for k, v in rouge_result.items()})\n",
    "\n",
    "    # --- BLEU ---\n",
    "    bleu_result = bleu.compute(predictions=candidates, references=references)\n",
    "    scores[\"BLEU\"] = round(bleu_result[\"bleu\"], 4)\n",
    "\n",
    "    # --- METEOR ---\n",
    "    meteor_result = meteor.compute(predictions=candidates, references=references)\n",
    "    scores[\"METEOR\"] = round(meteor_result[\"meteor\"], 4)\n",
    "\n",
    "    # --- BERTScore ---\n",
    "    bs_result = bertscore.compute(predictions=candidates, references=references, lang=\"en\")\n",
    "    scores[\"BERTScore_F1\"] = round(np.mean(bs_result[\"f1\"]), 4)\n",
    "\n",
    "    # --- Cosine Similarity (SBERT) ---\n",
    "    ref_embs = sbert_model.encode(references, convert_to_tensor=True)\n",
    "    cand_embs = sbert_model.encode(candidates, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(ref_embs, cand_embs).diagonal()\n",
    "    scores[\"Cosine_Similarity\"] = round(cosine_scores.cpu().numpy().mean(), 4)\n",
    "\n",
    "\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"multi_model_summaries.csv\")  # Update path\n",
    "\n",
    "# Define model-generated columns\n",
    "model_cols = ['t5_summary', 'pegasus_summary', 'flan-t5_summary', 'distilbart_summary']\n",
    "reference_col = 'summary'\n",
    "\n",
    "# Store metrics\n",
    "all_scores = []\n",
    "\n",
    "for model_col in model_cols:\n",
    "    print(f\"\\nüîç Evaluating {model_col} vs. human reference...\")\n",
    "    metrics = compute_all_metrics(\n",
    "        references=df[reference_col].astype(str).tolist(),\n",
    "        candidates=df[model_col].astype(str).tolist()\n",
    "    )\n",
    "    metrics[\"model\"] = model_col\n",
    "    all_scores.append(metrics)\n",
    "\n",
    "# Convert to DataFrame\n",
    "score_df = pd.DataFrame(all_scores)\n",
    "score_df = score_df[['model'] + [col for col in score_df.columns if col != 'model']]  # reorder\n",
    "print(score_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3208a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = ['ROUGE1', 'ROUGE2', 'BLEU', 'BERTScore_F1']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=score_df[metrics_to_plot])\n",
    "plt.title(\"Distribution of Evaluation Metrics Across Models\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove any unnamed columns\n",
    "score_df = score_df.loc[:, ~score_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Reshape for barplot\n",
    "melted_scores = score_df.melt(id_vars='model', var_name='Metric', value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "barplot = sns.barplot(\n",
    "    data=melted_scores,\n",
    "    x='Metric',\n",
    "    y='Score',\n",
    "    hue='model',\n",
    "    palette='Set2',\n",
    "    errorbar=None\n",
    ")\n",
    "\n",
    "plt.title(\"üìä Average Evaluation Scores by Metric and Model\", fontsize=16, weight='bold')\n",
    "plt.ylabel(\"Score\", fontsize=13)\n",
    "plt.xlabel(\"Evaluation Metric\", fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# Add score labels on top of each bar\n",
    "for container in barplot.containers:\n",
    "    barplot.bar_label(\n",
    "        container,\n",
    "        fmt='%.2f',\n",
    "        label_type='edge',\n",
    "        fontsize=10,\n",
    "        padding=3\n",
    "    )\n",
    "\n",
    "\n",
    "legend = plt.legend(\n",
    "    title=\"Model\",\n",
    "    loc='upper right',\n",
    "    bbox_to_anchor=(1.2, 1.03),\n",
    "    frameon=True,\n",
    "    fontsize=11,\n",
    "    title_fontsize=12\n",
    ")\n",
    "\n",
    "legend.get_frame().set_edgecolor('black')\n",
    "legend.get_frame().set_linewidth(1.2)\n",
    "legend.get_frame().set_facecolor('#f9f9f9')  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_word_counts(col):\n",
    "    return df[col].dropna().apply(lambda x: len(str(x).split()))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "colors = sns.color_palette(\"Set2\", len([reference_col] + model_cols))\n",
    "\n",
    "for idx, col in enumerate([reference_col] + model_cols):\n",
    "    word_counts = get_word_counts(col)\n",
    "    sns.kdeplot(\n",
    "        word_counts,\n",
    "        label=col.replace('_summary', '').capitalize(),\n",
    "        fill=True,\n",
    "        alpha=0.4,              \n",
    "        linewidth=1.8,\n",
    "        color=colors[idx],\n",
    "        bw_adjust=0.8           \n",
    "    )\n",
    "\n",
    "plt.title(\"üìö Word Count Distribution: Human vs AI-Generated Summaries\", fontsize=16, weight='bold')\n",
    "plt.xlabel(\"Word Count\", fontsize=13)\n",
    "plt.ylabel(\"Density\", fontsize=13)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Stylish legend box in top-right\n",
    "legend = plt.legend(\n",
    "    title=\"Summary Type\",\n",
    "    loc='upper right',\n",
    "    bbox_to_anchor=(1.15, 1.03),\n",
    "    frameon=True,\n",
    "    fontsize=12,\n",
    "    title_fontsize=14\n",
    ")\n",
    "legend.get_frame().set_edgecolor('black')\n",
    "legend.get_frame().set_linewidth(1.2)\n",
    "legend.get_frame().set_facecolor('#f9f9f9')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e684833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import textstat\n",
    "model_cols = ['summary', 't5_summary', 'pegasus_summary', 'flan-t5_summary', 'distilbart_summary']\n",
    "labels = ['Human', 'T5', 'PEGASUS', 'Flan-T5', 'DistilBART']\n",
    "\n",
    "def count_words(text): return len(word_tokenize(str(text)))\n",
    "def count_sentences(text): return len(sent_tokenize(str(text)))\n",
    "def count_chars(text): return len(str(text))\n",
    "def count_punct(text): return sum(1 for c in str(text) if c in '.,;:!?')\n",
    "\n",
    "results = {\n",
    "    'Model': labels,\n",
    "    'Word Count': [df[col].apply(count_words).mean() for col in model_cols],\n",
    "    'Sentence Count': [df[col].apply(count_sentences).mean() for col in model_cols],\n",
    "    'Character Count': [df[col].apply(count_chars).mean() for col in model_cols],\n",
    "    'Punctuation Count': [df[col].apply(count_punct).mean() for col in model_cols],\n",
    "    'Readability': [df[col].apply(lambda x: textstat.flesch_reading_ease(str(x))).mean() for col in model_cols],\n",
    "    'Compression Ratio': [df[col].apply(count_words).mean() / df['text'].apply(count_words).mean() for col in model_cols]\n",
    "}\n",
    "metrics_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de1127",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfcdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({\"axes.titleweight\": \"bold\", \"axes.labelweight\": \"bold\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df   = pd.read_csv(\"evaluation_scores.csv\").drop(columns=['Unnamed: 0'])\n",
    "metrics_df = pd.read_csv(\"metrics.csv\").drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.rename(columns={\"Model\": \"Model Type\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_pairs = [\n",
    "    (\"Word Count\",          \"Avg¬†Words\"),\n",
    "    (\"Sentence Count\",      \"Avg¬†Sentences\"),\n",
    "    (\"Character Count\",     \"Avg¬†Chars\"),\n",
    "    (\"Punctuation Count\",   \"Avg¬†Punct.\"),\n",
    "    (\"Readability\",         \"Flesch¬†Ease\"),\n",
    "    (\"Compression Ratio\",   \"Comp.¬†Ratio\")\n",
    "]\n",
    "palette = sns.color_palette(\"Set2\", n_colors=len(metrics_df))\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (col, ylab) in zip(axes, metric_pairs):\n",
    "    bars = sns.barplot(\n",
    "        data=metrics_df, x=\"Model Type\", y=col, hue=\"Model Type\",\n",
    "        palette=palette, legend=False, ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{col}¬†‚Äì Human vs‚ÄØAI\", fontsize=13, pad=8)\n",
    "    ax.set_ylabel(ylab);  ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis='x', rotation=10)\n",
    "    # value labels\n",
    "    for p in bars.patches:\n",
    "        h = p.get_height(); x = p.get_x()+p.get_width()/2\n",
    "        ax.annotate(f\"{h:.1f}\", (x, h), ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\n",
    "    ax.grid(alpha=.25, linestyle=\"--\")\n",
    "\n",
    "fig.suptitle(\"Structural / Readability Metrics ‚Äì Comparison Across Summaries\", fontsize=15, y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83227591",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_cols = ['ROUGE1', 'ROUGE2', 'ROUGEL', 'BLEU',\n",
    "             'BERTScore_F1', 'Cosine_Similarity']\n",
    "plt.figure(figsize=(8,6))\n",
    "corr = eval_df[heat_cols].corr(method='spearman')\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Spearman Correlation ‚Äì Automatic Metrics\", fontsize=14, pad=10, weight='bold')\n",
    "plt.tight_layout();  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_corpus = \" \".join(df['text'].astype(str))\n",
    "sum_corpus = \" \".join(df['summary'].astype(str))\n",
    "\n",
    "for title, corpus in [('Full Text','text_corpus'), ('Human Summary','sum_corpus')]:\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate(locals()[corpus])\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title(f\"Word Cloud: {title}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46651ce1",
   "metadata": {},
   "source": [
    "# Jaccard Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23472f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textdistance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_summaries(df, model_columns, reference_col='summary', source_col='text'):\n",
    "    records = []\n",
    "\n",
    "    for model in model_columns:\n",
    "        jaccard_scores = []\n",
    "        levenshtein_dists = []\n",
    "        compression_ratios = []\n",
    "        retentions = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            model_text = str(row[model])\n",
    "            human_text = str(row[reference_col])\n",
    "            source_text = str(row[source_col])\n",
    "\n",
    "            # Jaccard Similarity\n",
    "            vectorizer = CountVectorizer(binary=True)\n",
    "            try:\n",
    "                vecs = vectorizer.fit_transform([model_text, human_text]).toarray()\n",
    "                jaccard = sum(vecs[0] & vecs[1]) / sum(vecs[0] | vecs[1]) if sum(vecs[0] | vecs[1]) != 0 else 0\n",
    "            except:\n",
    "                jaccard = 0\n",
    "            jaccard_scores.append(jaccard)\n",
    "\n",
    "            # Levenshtein Distance\n",
    "            lev_dist = textdistance.levenshtein(model_text, human_text)\n",
    "            levenshtein_dists.append(lev_dist)\n",
    "\n",
    "            # Compression Ratio\n",
    "            source_len = len(source_text.split())\n",
    "            model_len = len(model_text.split())\n",
    "            compression = model_len / source_len if source_len else 0\n",
    "            compression_ratios.append(compression)\n",
    "\n",
    "            # Retention (placeholder: using ROUGE-L if available, otherwise 0)\n",
    "            retention = row.get('ROUGE_L', 0)\n",
    "            retentions.append(retention)\n",
    "\n",
    "        # Aggregate stats for each model\n",
    "        records.append({\n",
    "            'Model': model,\n",
    "            'Jaccard Similarity': round(sum(jaccard_scores)/len(jaccard_scores), 4),\n",
    "            'Levenshtein Distance': round(sum(levenshtein_dists)/len(levenshtein_dists), 2),\n",
    "            'Compression Ratio': round(sum(compression_ratios)/len(compression_ratios), 4),\n",
    "            'Retention': round(sum(retentions)/len(retentions), 4)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage:\n",
    "df = pd.read_csv(r\"C:\\Users\\sapan\\Desktop\\laptop4\\multi_model_summaries.csv\", engine=\"python\", on_bad_lines='skip')\n",
    "model_cols = ['t5_summary', 'pegasus_summary', 'flan-t5_summary', 'distilbart_summary']\n",
    "metrics_df = evaluate_summaries(df, model_cols)\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_bars(metrics_df, metric_name):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(data=metrics_df, x='Model', y=metric_name, palette='viridis')\n",
    "    plt.title(f'{metric_name} across Models vs Human')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add value labels\n",
    "    for index, row in metrics_df.iterrows():\n",
    "        plt.text(index, row[metric_name] + 0.01, f\"{row[metric_name]:.4f}\", ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot each metric\n",
    "for metric in ['Jaccard Similarity', 'Levenshtein Distance', 'Compression Ratio', 'Retention']:\n",
    "    plot_metric_bars(metrics_df, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebdb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "def run_anova_all_metrics(df, group_col='Model'):\n",
    "    results = []\n",
    "\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "    for metric in numeric_cols:\n",
    "        # Group values by model\n",
    "        grouped = df.groupby(group_col)[metric].apply(list)\n",
    "\n",
    "        # Only run ANOVA if all groups have at least 2 values\n",
    "        if all(len(vals) >= 2 for vals in grouped):\n",
    "            f_stat, p_val = f_oneway(*grouped)\n",
    "            results.append({\n",
    "                'Metric': metric,\n",
    "                'F-statistic': round(f_stat, 4),\n",
    "                'p-value': round(p_val, 4)\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                'Metric': metric,\n",
    "                'F-statistic': None,\n",
    "                'p-value': None,\n",
    "                'Note': 'Insufficient data in one or more groups'\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage:\n",
    "metrics_df = pd.read_csv(\"metrics.csv\")  # Replace with your actual path\n",
    "anova_results = run_anova_all_metrics(metrics_df)\n",
    "print(anova_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
