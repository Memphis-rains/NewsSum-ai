{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0715b853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training on: cuda\n",
      "🚀 Starting CNN/DailyMail Fine-tuning Pipeline\n",
      "Timestamp: 2025-08-23 21:37:03.155128\n",
      "Loading dataset...\n",
      "Setting up sentiment analysis pipelines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Emotion model not available, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "🚀 Fine-tuning t5...\n",
      "Training samples: 18000\n",
      "Validation samples: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:03<00:00, 562.56 examples/s]\n",
      "C:\\Users\\sapan\\AppData\\Local\\Temp\\ipykernel_124504\\4258464920.py:181: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18000' max='18000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18000/18000 23:24:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.808300</td>\n",
       "      <td>0.691227</td>\n",
       "      <td>26.340000</td>\n",
       "      <td>11.870000</td>\n",
       "      <td>21.460000</td>\n",
       "      <td>21.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.925800</td>\n",
       "      <td>0.816013</td>\n",
       "      <td>24.980000</td>\n",
       "      <td>10.340000</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>20.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ t5 training completed and saved!\n",
      "\n",
      "📊 Processing test articles with t5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t5 inference:  50%|█████     | 5/10 [00:38<00:36,  7.34s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "t5 inference: 100%|██████████| 10/10 [01:13<00:00,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 10 results to ./results_csv/t5_results.csv\n",
      "\n",
      "🎉 Pipeline completed! Check results in ./results_csv/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MODEL_DICT = {\n",
    "    \"t5\":         \"t5-base\",\n",
    "}\n",
    "\n",
    "OUTPUT_ROOT = \"./finetuned_cnn_dm_sentiment\"\n",
    "RESULTS_ROOT = \"./results_csv\"\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
    "\n",
    "MAX_INPUT_LEN = 512\n",
    "MAX_TARGET_LEN = 128\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 5e-5\n",
    "TRAIN_SIZE = 20000  # Made configurable\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ Training on: {device}\")\n",
    "\n",
    "# ---------------- LOAD DATASET ----------------\n",
    "def load_and_prepare_dataset():\n",
    "    \"\"\"Load and prepare the CNN/DailyMail dataset\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "    dataset = dataset[\"train\"].select(range(TRAIN_SIZE))\n",
    "    return dataset\n",
    "\n",
    "# ---------------- METRIC ----------------\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"Compute ROUGE metrics for evaluation\"\"\"\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up predictions and labels\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "# ---------------- SENTIMENT ANALYSIS SETUP ----------------\n",
    "def setup_sentiment_pipelines():\n",
    "    \"\"\"Initialize all sentiment analysis pipelines\"\"\"\n",
    "    print(\"Setting up sentiment analysis pipelines...\")\n",
    "    \n",
    "    pipelines = {}\n",
    "    try:\n",
    "        pipelines['polarity'] = pipeline(\"sentiment-analysis\", \n",
    "                                        model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    except:\n",
    "        pipelines['polarity'] = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    try:\n",
    "        pipelines['finegrained'] = pipeline(\"sentiment-analysis\", \n",
    "                                           model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    except:\n",
    "        print(\"Warning: Fine-grained sentiment model not available, skipping...\")\n",
    "        pipelines['finegrained'] = None\n",
    "    \n",
    "    try:\n",
    "        pipelines['emotion'] = pipeline(\"text-classification\", \n",
    "                                       model=\"j-hartmann/emotion-english-distilroberta-base\", \n",
    "                                       return_all_scores=True)\n",
    "    except:\n",
    "        print(\"Warning: Emotion model not available, skipping...\")\n",
    "        pipelines['emotion'] = None\n",
    "    \n",
    "    try:\n",
    "        pipelines['zero_shot'] = pipeline(\"zero-shot-classification\", \n",
    "                                         model=\"facebook/bart-large-mnli\")\n",
    "    except:\n",
    "        print(\"Warning: Zero-shot model not available, skipping...\")\n",
    "        pipelines['zero_shot'] = None\n",
    "    \n",
    "    return pipelines\n",
    "\n",
    "# Labels for zero-shot classification\n",
    "INTENT_LABELS = [\"informative\", \"warning\", \"opinion\", \"breaking news\", \"analysis\"]\n",
    "ASPECT_LABELS = [\"economy\", \"politics\", \"health\", \"sports\", \"technology\", \"entertainment\"]\n",
    "\n",
    "# ---------------- TRAINING FUNCTION ----------------\n",
    "def train_and_save(model_key, model_name, dataset, sentiment_pipelines):\n",
    "    \"\"\"Train a model and return inference function\"\"\"\n",
    "    print(f\"\\n🚀 Fine-tuning {model_key}...\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        # Add pad token if missing\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "        \n",
    "        def preprocess(batch):\n",
    "            inputs = batch[\"article\"]\n",
    "            if \"t5\" in model_key:\n",
    "                inputs = [\"summarize: \" + text for text in inputs]\n",
    "            \n",
    "            model_inputs = tokenizer(\n",
    "                inputs, \n",
    "                max_length=MAX_INPUT_LEN, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            \n",
    "            labels = tokenizer(\n",
    "                batch[\"highlights\"], \n",
    "                max_length=MAX_TARGET_LEN, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            \n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        # Split dataset\n",
    "        split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        ds_train = split_dataset[\"train\"]\n",
    "        ds_val = split_dataset[\"test\"]\n",
    "        \n",
    "        print(f\"Training samples: {len(ds_train)}\")\n",
    "        print(f\"Validation samples: {len(ds_val)}\")\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        tokenized_train = ds_train.map(preprocess, batched=True, remove_columns=ds_train.column_names)\n",
    "        tokenized_val = ds_val.map(preprocess, batched=True, remove_columns=ds_val.column_names)\n",
    "        \n",
    "        collator = DataCollatorForSeq2Seq(tokenizer, model)\n",
    "        \n",
    "        args = Seq2SeqTrainingArguments(\n",
    "            output_dir=f\"{OUTPUT_ROOT}/{model_key}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            weight_decay=0.01,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            logging_dir=f\"{OUTPUT_ROOT}/{model_key}/logs\",\n",
    "            report_to=\"none\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_rouge1\",\n",
    "            greater_is_better=True,\n",
    "        )\n",
    "        \n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collator,\n",
    "            compute_metrics=lambda x: compute_metrics(x, tokenizer),\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model and tokenizer\n",
    "        trainer.save_model(f\"{OUTPUT_ROOT}/{model_key}\")\n",
    "        tokenizer.save_pretrained(f\"{OUTPUT_ROOT}/{model_key}\")\n",
    "        \n",
    "        # Save training metrics\n",
    "        training_history = trainer.state.log_history\n",
    "        with open(f\"{OUTPUT_ROOT}/{model_key}/training_history.json\", \"w\") as f:\n",
    "            json.dump(training_history, f, indent=2)\n",
    "        \n",
    "        print(f\"✅ {model_key} training completed and saved!\")\n",
    "        \n",
    "        # Create inference function\n",
    "        def summarize_with_sentiments(article_text):\n",
    "            try:\n",
    "                prefix = \"summarize: \" if \"t5\" in model_key else \"\"\n",
    "                inputs = tokenizer(\n",
    "                    prefix + article_text, \n",
    "                    return_tensors=\"pt\", \n",
    "                    truncation=True, \n",
    "                    max_length=MAX_INPUT_LEN\n",
    "                )\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    ids = model.generate(\n",
    "                        **inputs, \n",
    "                        max_length=MAX_TARGET_LEN, \n",
    "                        num_beams=4, \n",
    "                        early_stopping=True,\n",
    "                        no_repeat_ngram_size=2\n",
    "                    )\n",
    "                \n",
    "                summary = tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "                trainer.save_model(f\"{OUTPUT_ROOT}/{model_key}\")\n",
    "                tokenizer.save_pretrained(f\"{OUTPUT_ROOT}/{model_key}\")\n",
    "                # Perform sentiment analysis\n",
    "                result = {\"summary\": summary, \"model\": model_key}\n",
    "                \n",
    "                # Polarity analysis\n",
    "                if sentiment_pipelines['polarity']:\n",
    "                    try:\n",
    "                        polarity = sentiment_pipelines['polarity'](summary)[0]\n",
    "                        result[\"polarity\"] = polarity\n",
    "                    except Exception as e:\n",
    "                        print(f\"Polarity analysis failed: {e}\")\n",
    "                        result[\"polarity\"] = {\"label\": \"UNKNOWN\", \"score\": 0.0}\n",
    "                \n",
    "                # Fine-grained sentiment\n",
    "                if sentiment_pipelines['finegrained']:\n",
    "                    try:\n",
    "                        finegrained = sentiment_pipelines['finegrained'](summary)[0]\n",
    "                        result[\"finegrained\"] = finegrained\n",
    "                    except Exception as e:\n",
    "                        print(f\"Fine-grained analysis failed: {e}\")\n",
    "                        result[\"finegrained\"] = {\"label\": \"UNKNOWN\", \"score\": 0.0}\n",
    "                \n",
    "                # Emotion analysis\n",
    "                if sentiment_pipelines['emotion']:\n",
    "                    try:\n",
    "                        emotions = sentiment_pipelines['emotion'](summary)[0]\n",
    "                        result[\"emotions\"] = emotions\n",
    "                    except Exception as e:\n",
    "                        print(f\"Emotion analysis failed: {e}\")\n",
    "                        result[\"emotions\"] = []\n",
    "                \n",
    "                # Intent classification\n",
    "                if sentiment_pipelines['zero_shot']:\n",
    "                    try:\n",
    "                        intent = sentiment_pipelines['zero_shot'](summary, candidate_labels=INTENT_LABELS)\n",
    "                        result[\"intent\"] = intent\n",
    "                    except Exception as e:\n",
    "                        print(f\"Intent classification failed: {e}\")\n",
    "                        result[\"intent\"] = {\"labels\": [], \"scores\": []}\n",
    "                \n",
    "                # Aspect classification\n",
    "                if sentiment_pipelines['zero_shot']:\n",
    "                    try:\n",
    "                        aspects = sentiment_pipelines['zero_shot'](summary, candidate_labels=ASPECT_LABELS)\n",
    "                        result[\"aspects\"] = aspects\n",
    "                    except Exception as e:\n",
    "                        print(f\"Aspect classification failed: {e}\")\n",
    "                        result[\"aspects\"] = {\"labels\": [], \"scores\": []}\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in summarization: {e}\")\n",
    "                return {\n",
    "                    \"summary\": \"Error generating summary\",\n",
    "                    \"model\": model_key,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "        \n",
    "        return summarize_with_sentiments\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error training {model_key}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------- MAIN EXECUTION ----------------\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting CNN/DailyMail Fine-tuning Pipeline\")\n",
    "    print(f\"Timestamp: {datetime.now()}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_and_prepare_dataset()\n",
    "    \n",
    "    # Setup sentiment pipelines\n",
    "    sentiment_pipelines = setup_sentiment_pipelines()\n",
    "    \n",
    "    # Train models and collect inference functions\n",
    "    model_functions = {}\n",
    "    for key, name in MODEL_DICT.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        summarize_fn = train_and_save(key, name, dataset, sentiment_pipelines)\n",
    "        if summarize_fn:\n",
    "            model_functions[key] = summarize_fn\n",
    "            # Clear memory after each model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Test on sample articles\n",
    "    sample_articles = [dataset[i][\"article\"] for i in range(10)]  # Reduced for faster testing\n",
    "    \n",
    "    for key, fn in model_functions.items():\n",
    "        results = []\n",
    "        print(f\"\\n📊 Processing test articles with {key}...\")\n",
    "        \n",
    "        for i, article in enumerate(tqdm(sample_articles, desc=f\"{key} inference\")):\n",
    "            try:\n",
    "                res = fn(article)\n",
    "                res[\"article_id\"] = i\n",
    "                res[\"article_snippet\"] = article[:200] + \"...\" if len(article) > 200 else article\n",
    "                results.append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {i} with {key}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Save results\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            output_file = f\"{RESULTS_ROOT}/{key}_results.csv\"\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"✅ Saved {len(results)} results to {output_file}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Pipeline completed! Check results in {RESULTS_ROOT}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9224439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model j-hartmann/emotion-english-distilroberta-base with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>, <class 'transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4990, in from_pretrained\n    config, torch_dtype, dtype_orig = _get_torch_dtype(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1387, in _get_torch_dtype\n    state_dict = load_state_dict(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 562, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1622, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 5074, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 5340, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 562, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1622, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2929, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1396, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 205, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 1269, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'torch_dtype')\n\nwhile loading with RobertaForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4990, in from_pretrained\n    config, torch_dtype, dtype_orig = _get_torch_dtype(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1387, in _get_torch_dtype\n    state_dict = load_state_dict(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 562, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1622, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 5074, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 5340, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 562, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1622, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with TFRobertaForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2929, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1396, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 205, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 1269, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'torch_dtype')\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m pipelines \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     15\u001b[0m pipelines[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinegrained\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m                                     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m pipelines[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mj-hartmann/emotion-english-distilroberta-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mreturn_all_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m pipelines[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero_shot\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m                                   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-mnli\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m INTENT_LABELS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformative\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopinion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbreaking news\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1008\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m-> 1008\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m   1009\u001b[0m         adapter_path \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[0;32m   1010\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m   1011\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   1012\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m   1013\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1016\u001b[0m     )\n\u001b[0;32m   1018\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py:332\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    331\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model j-hartmann/emotion-english-distilroberta-base with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>, <class 'transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4990, in from_pretrained\n    config, torch_dtype, dtype_orig = _get_torch_dtype(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1387, in _get_torch_dtype\n    state_dict = load_state_dict(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 562, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1622, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 5074, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 5340, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 562, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1622, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2929, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1396, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 205, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 1269, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'torch_dtype')\n\nwhile loading with RobertaForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4990, in from_pretrained\n    config, torch_dtype, dtype_orig = _get_torch_dtype(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1387, in _get_torch_dtype\n    state_dict = load_state_dict(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 562, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1622, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 5074, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 5340, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_utils.py\", line 562, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1622, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with TFRobertaForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2929, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1396, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 205, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"c:\\Users\\sapan\\anaconda3\\envs\\py10-gpu-tf\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 1269, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'torch_dtype')\n\n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# ---------------- PATHS ----------------\n",
    "MODEL_PATH = \"./finetuned_cnn_dm_sentiment/t5\"\n",
    "\n",
    "# ---------------- LOAD MODEL ----------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "\n",
    "# ---------------- SENTIMENT / CLASSIFICATION PIPELINES ----------------\n",
    "pipelines = {}\n",
    "\n",
    "pipelines['finegrained'] = pipeline(\"sentiment-analysis\",\n",
    "                                    model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "pipelines['emotion'] = pipeline(\"text-classification\",\n",
    "                                model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "                                return_all_scores=True)\n",
    "pipelines['zero_shot'] = pipeline(\"zero-shot-classification\",\n",
    "                                  model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "INTENT_LABELS = [\"informative\", \"warning\", \"opinion\", \"breaking news\", \"analysis\"]\n",
    "ASPECT_LABELS = [\"economy\", \"politics\", \"health\", \"sports\", \"technology\", \"entertainment\"]\n",
    "\n",
    "# ---------------- SUMMARIZE + SENTIMENT ----------------\n",
    "def summarize_with_sentiments(text, max_input_len=512, max_output_len=128):\n",
    "    \"\"\"Generate a summary and run sentiment/emotion/zero-shot analysis\"\"\"\n",
    "\n",
    "    # Add prefix for T5\n",
    "    if \"t5\" in MODEL_PATH.lower():\n",
    "        text = \"summarize: \" + text\n",
    "\n",
    "    # Encode input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=max_input_len).to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_output_len,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Build result dictionary\n",
    "    result = {\"summary\": summary, \"model\": \"t5\"}\n",
    "\n",
    "    # Run sentiment / classification\n",
    "    try:\n",
    "        result[\"polarity\"] = pipelines['polarity'](summary)[0]\n",
    "    except:\n",
    "        result[\"polarity\"] = {\"label\": \"UNKNOWN\", \"score\": 0.0}\n",
    "\n",
    "    try:\n",
    "        result[\"finegrained\"] = pipelines['finegrained'](summary)[0]\n",
    "    except:\n",
    "        result[\"finegrained\"] = {\"label\": \"UNKNOWN\", \"score\": 0.0}\n",
    "\n",
    "    try:\n",
    "        result[\"emotions\"] = pipelines['emotion'](summary)[0]\n",
    "    except:\n",
    "        result[\"emotions\"] = []\n",
    "\n",
    "    try:\n",
    "        result[\"intent\"] = pipelines['zero_shot'](summary, candidate_labels=INTENT_LABELS)\n",
    "    except:\n",
    "        result[\"intent\"] = {\"labels\": [], \"scores\": []}\n",
    "\n",
    "    try:\n",
    "        result[\"aspects\"] = pipelines['zero_shot'](summary, candidate_labels=ASPECT_LABELS)\n",
    "    except:\n",
    "        result[\"aspects\"] = {\"labels\": [], \"scores\": []}\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b907454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "The U.S. economy showed stronger-than-expected growth last quarter, \n",
      "with consumer spending remaining resilient despite high inflation. \n",
      "Experts believe the Federal Reserve may reconsider its stance ...\n",
      "Generated Summary: The U.S. economy showed stronger-than-expected growth last quarter . Consumer spending remained resilient despite high inflation.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "The U.S. economy showed stronger-than-expected growth last quarter, \n",
    "with consumer spending remaining resilient despite high inflation. \n",
    "Experts believe the Federal Reserve may reconsider its stance on interest rate hikes \n",
    "as signs of cooling inflation begin to emerge.\n",
    "\"\"\"\n",
    "\n",
    "summary = generate_summary(sample_text)\n",
    "print(\"Input:\", sample_text[:200] + \"...\")\n",
    "print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b54fccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from ./finetuned_cnn_dm_sentiment/t5...\n",
      "✅ Model loaded successfully!\n",
      "Setting up sentiment analysis pipelines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentiment pipelines loaded!\n",
      "Processing article...\n",
      "\n",
      "============================================================\n",
      "FINE-TUNED MODEL RESULTS\n",
      "============================================================\n",
      "\n",
      "Original Article Length: 154 words\n",
      "Summary Length: 47 words\n",
      "\n",
      "SUMMARY:\n",
      "'The new lithium-metal battery can store up to 10 times more energy than traditional batteries . Commercial production is expected to begin within two years, potentially revolutionizing the electric vehicle industry. The research was funded by the Department of Energy and published in the journal Nature Energy.'\n",
      "\n",
      "POLARITY SENTIMENT:\n",
      "Label: POSITIVE\n",
      "Confidence: 0.971\n",
      "\n",
      "FINE-GRAINED SENTIMENT:\n",
      "Rating: 5 stars\n",
      "Confidence: 0.534\n",
      "\n",
      "INTENT CLASSIFICATION:\n",
      "Primary Intent: informative (confidence: 0.493)\n",
      "\n",
      "ASPECT CLASSIFICATION:\n",
      "Primary Aspect: technology (confidence: 0.909)\n",
      "\n",
      "============================================================\n",
      "Result saved to: sample_inference_result.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    ")\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"./finetuned_cnn_dm_sentiment/t5\"  # Path where your model is saved\n",
    "MAX_INPUT_LEN = 512\n",
    "MAX_TARGET_LEN = 128\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class FineTunedSummarizer:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Initialize the fine-tuned summarizer\"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.load_model()\n",
    "        self.setup_sentiment_pipelines()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the fine-tuned model and tokenizer\"\"\"\n",
    "        print(f\"Loading model from {self.model_path}...\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, use_fast=True)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_path).to(device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "    \n",
    "    def setup_sentiment_pipelines(self):\n",
    "        \"\"\"Initialize sentiment analysis pipelines\"\"\"\n",
    "        print(\"Setting up sentiment analysis pipelines...\")\n",
    "        \n",
    "        self.sentiment_pipelines = {}\n",
    "        \n",
    "        try:\n",
    "            self.sentiment_pipelines['polarity'] = pipeline(\n",
    "                \"sentiment-analysis\", \n",
    "                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "            )\n",
    "        except:\n",
    "            self.sentiment_pipelines['polarity'] = pipeline(\"sentiment-analysis\")\n",
    "        \n",
    "        try:\n",
    "            self.sentiment_pipelines['finegrained'] = pipeline(\n",
    "                \"sentiment-analysis\", \n",
    "                model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "            )\n",
    "        except:\n",
    "            self.sentiment_pipelines['finegrained'] = None\n",
    "        \n",
    "        try:\n",
    "            self.sentiment_pipelines['emotion'] = pipeline(\n",
    "                \"text-classification\", \n",
    "                model=\"j-hartmann/emotion-english-distilroberta-base\", \n",
    "                return_all_scores=True\n",
    "            )\n",
    "        except:\n",
    "            self.sentiment_pipelines['emotion'] = None\n",
    "        \n",
    "        try:\n",
    "            self.sentiment_pipelines['zero_shot'] = pipeline(\n",
    "                \"zero-shot-classification\", \n",
    "                model=\"facebook/bart-large-mnli\"\n",
    "            )\n",
    "        except:\n",
    "            self.sentiment_pipelines['zero_shot'] = None\n",
    "        \n",
    "        print(\"✅ Sentiment pipelines loaded!\")\n",
    "    \n",
    "    def summarize_with_sentiments(self, article_text):\n",
    "        \"\"\"Generate summary with sentiment analysis\"\"\"\n",
    "        try:\n",
    "            # Prepare input (T5 needs \"summarize:\" prefix)\n",
    "            prefix = \"summarize: \"\n",
    "            inputs = self.tokenizer(\n",
    "                prefix + article_text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=MAX_INPUT_LEN,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move inputs to the same device as model\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate summary\n",
    "            with torch.no_grad():\n",
    "                ids = self.model.generate(\n",
    "                    **inputs, \n",
    "                    max_length=MAX_TARGET_LEN, \n",
    "                    num_beams=4, \n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the summary\n",
    "            summary = self.tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Prepare result dictionary\n",
    "            result = {\n",
    "                \"summary\": summary,\n",
    "                \"model\": \"t5-finetuned\",\n",
    "                \"article_length\": len(article_text.split())\n",
    "            }\n",
    "            \n",
    "            # Perform sentiment analysis on the summary\n",
    "            self._analyze_sentiments(summary, result)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in summarization: {e}\")\n",
    "            return {\n",
    "                \"summary\": \"Error generating summary\",\n",
    "                \"model\": \"t5-finetuned\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _analyze_sentiments(self, summary, result):\n",
    "        \"\"\"Perform various sentiment analyses on the summary\"\"\"\n",
    "        # Intent and aspect labels\n",
    "        INTENT_LABELS = [\"informative\", \"warning\", \"opinion\", \"breaking news\", \"analysis\"]\n",
    "        ASPECT_LABELS = [\"economy\", \"politics\", \"health\", \"sports\", \"technology\", \"entertainment\"]\n",
    "        \n",
    "        # Polarity analysis\n",
    "        if self.sentiment_pipelines['polarity']:\n",
    "            try:\n",
    "                polarity = self.sentiment_pipelines['polarity'](summary)[0]\n",
    "                result[\"polarity\"] = polarity\n",
    "            except Exception as e:\n",
    "                result[\"polarity\"] = {\"label\": \"UNKNOWN\", \"score\": 0.0}\n",
    "        \n",
    "        # Fine-grained sentiment\n",
    "        if self.sentiment_pipelines['finegrained']:\n",
    "            try:\n",
    "                finegrained = self.sentiment_pipelines['finegrained'](summary)[0]\n",
    "                result[\"finegrained\"] = finegrained\n",
    "            except Exception as e:\n",
    "                result[\"finegrained\"] = {\"label\": \"UNKNOWN\", \"score\": 0.0}\n",
    "        \n",
    "        # Emotion analysis\n",
    "        if self.sentiment_pipelines['emotion']:\n",
    "            try:\n",
    "                emotions = self.sentiment_pipelines['emotion'](summary)[0]\n",
    "                result[\"emotions\"] = emotions\n",
    "            except Exception as e:\n",
    "                result[\"emotions\"] = []\n",
    "        \n",
    "        # Intent classification\n",
    "        if self.sentiment_pipelines['zero_shot']:\n",
    "            try:\n",
    "                intent = self.sentiment_pipelines['zero_shot'](summary, candidate_labels=INTENT_LABELS)\n",
    "                result[\"intent\"] = intent\n",
    "            except Exception as e:\n",
    "                result[\"intent\"] = {\"labels\": [], \"scores\": []}\n",
    "        \n",
    "        # Aspect classification\n",
    "        if self.sentiment_pipelines['zero_shot']:\n",
    "            try:\n",
    "                aspects = self.sentiment_pipelines['zero_shot'](summary, candidate_labels=ASPECT_LABELS)\n",
    "                result[\"aspects\"] = aspects\n",
    "            except Exception as e:\n",
    "                result[\"aspects\"] = {\"labels\": [], \"scores\": []}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the fine-tuned model\"\"\"\n",
    "    \n",
    "    # Initialize the summarizer\n",
    "    summarizer = FineTunedSummarizer(MODEL_PATH)\n",
    "    \n",
    "    # Sample news article (you can replace this with your own text)\n",
    "    sample_article = \"\"\"\n",
    "    Scientists at MIT have developed a revolutionary new battery technology that could dramatically extend the range of electric vehicles. The new lithium-metal battery can store up to 10 times more energy than traditional lithium-ion batteries while charging in just 10 minutes. The research team, led by Dr. Sarah Johnson, has spent five years developing this breakthrough technology. Initial tests show that electric vehicles equipped with these batteries could travel over 1,000 miles on a single charge. The technology uses a novel polymer coating that prevents the formation of dendrites, which have been a major obstacle in lithium-metal battery development. Major automakers including Tesla, Ford, and General Motors have already expressed interest in licensing the technology. Commercial production is expected to begin within two years, potentially revolutionizing the electric vehicle industry and accelerating the transition away from fossil fuels. The research was funded by the Department of Energy and published in the journal Nature Energy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate summary with sentiment analysis\n",
    "    print(\"Processing article...\")\n",
    "    result = summarizer.summarize_with_sentiments(sample_article)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINE-TUNED MODEL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOriginal Article Length: {len(sample_article.split())} words\")\n",
    "    print(f\"Summary Length: {len(result['summary'].split())} words\")\n",
    "    \n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    print(f\"'{result['summary']}'\")\n",
    "    \n",
    "    if 'polarity' in result:\n",
    "        print(f\"\\nPOLARITY SENTIMENT:\")\n",
    "        print(f\"Label: {result['polarity']['label']}\")\n",
    "        print(f\"Confidence: {result['polarity']['score']:.3f}\")\n",
    "    \n",
    "    if 'finegrained' in result and result['finegrained']:\n",
    "        print(f\"\\nFINE-GRAINED SENTIMENT:\")\n",
    "        print(f\"Rating: {result['finegrained']['label']}\")\n",
    "        print(f\"Confidence: {result['finegrained']['score']:.3f}\")\n",
    "    \n",
    "    if 'emotions' in result and result['emotions']:\n",
    "        print(f\"\\nTOP 3 EMOTIONS:\")\n",
    "        sorted_emotions = sorted(result['emotions'], key=lambda x: x['score'], reverse=True)[:3]\n",
    "        for emotion in sorted_emotions:\n",
    "            print(f\"- {emotion['label']}: {emotion['score']:.3f}\")\n",
    "    \n",
    "    if 'intent' in result and result['intent'].get('labels'):\n",
    "        print(f\"\\nINTENT CLASSIFICATION:\")\n",
    "        top_intent = result['intent']['labels'][0]\n",
    "        top_score = result['intent']['scores'][0]\n",
    "        print(f\"Primary Intent: {top_intent} (confidence: {top_score:.3f})\")\n",
    "    \n",
    "    if 'aspects' in result and result['aspects'].get('labels'):\n",
    "        print(f\"\\nASPECT CLASSIFICATION:\")\n",
    "        top_aspect = result['aspects']['labels'][0]\n",
    "        top_score = result['aspects']['scores'][0]\n",
    "        print(f\"Primary Aspect: {top_aspect} (confidence: {top_score:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Save result to JSON file\n",
    "    output_file = \"sample_inference_result.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    print(f\"Result saved to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10-gpu-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
